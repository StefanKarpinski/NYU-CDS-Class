{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single neural network layer using Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TextParse\n",
    "using DataFrames\n",
    "cols, colnames = TextParse.csvread(\"Apple_Golden_1.dat\",'\\t')\n",
    "apples_1 = DataFrame(Dict(name=>col for (name, col) in zip(colnames, cols)))\n",
    "cols, colnames = TextParse.csvread(\"Apple_Golden_2.dat\",'\\t')\n",
    "apples_2 = DataFrame(Dict(name=>col for (name, col) in zip(colnames, cols)))\n",
    "cols, colnames = TextParse.csvread(\"Apple_Golden_3.dat\",'\\t')\n",
    "apples_3 = DataFrame(Dict(name=>col for (name, col) in zip(colnames, cols)))\n",
    "cols, colnames = TextParse.csvread(\"Banana.dat\",'\\t')\n",
    "bananas = DataFrame(Dict(name=>col for (name, col) in zip(colnames, cols)))\n",
    "cols, colnames = TextParse.csvread(\"Grape_White.dat\",'\\t')\n",
    "grapes_1 = DataFrame(Dict(name=>col for (name, col) in zip(colnames, cols)))\n",
    "cols, colnames = TextParse.csvread(\"Grape_White_2.dat\",'\\t')\n",
    "grapes_2 = DataFrame(Dict(name=>col for (name, col) in zip(colnames, cols)))\n",
    "\n",
    "apples = vcat(apples_1, apples_2, apples_3)\n",
    "grapes = vcat(grapes_1, grapes_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bananas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = 4 #red\n",
    "col2 = 1 #blue\n",
    "\n",
    "x_apples  = [ [apples_1[i, col1], apples_1[i, col2]] for i in 1:size(apples_1)[1] ]\n",
    "append!(x_apples, [ [apples_2[i, col1], apples_2[i, col2]] for i in 1:size(apples_2)[1] ])\n",
    "append!(x_apples, [ [apples_3[i, col1], apples_3[i, col2]] for i in 1:size(apples_3)[1] ])\n",
    "\n",
    "x_bananas = [ [bananas[i, col1], bananas[i, col2]] for i in 1:size(bananas)[1] ]\n",
    "\n",
    "x_grapes = [ [grapes_1[i, col1], grapes_1[i, col2]] for i in 1:size(grapes_1)[1] ]\n",
    "append!(x_grapes, [ [grapes_2[i, col1], grapes_2[i, col2]] for i in 1:size(grapes_2)[1] ])\n",
    "\n",
    "xs = vcat(x_apples, x_bananas, x_grapes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now we wish to classify the three types of fruit, so we will use as output *one-hot vectors*. Effectively, the first neuron learns whether (1) or not (0) the data corresponds to an apple, the second whether (1) or not (0) it corresponds to a banana, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ones(length(x_apples)); 2*ones(length(x_bananas)); 3*ones(length(x_grapes))];\n",
    "\n",
    "ys = [onehot(label, 1:3) for label in labels]  # onehotbatch(labels, 1:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot(1, 1:3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is in `xs` and the one-hot vectors are in `ys`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single layer in Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that there are two pieces of input data. Then the network has 2 input neurons and 3 output neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"draw_neural_net.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()\n",
    "draw_layer(1, 1, 2, 3, 0.2)\n",
    "plot!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(2, 3, σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 6 lines in the figure denotes a weight of the neuron on the right, taking as input the output of the neuron on the left. These weights are collected in the **matrix** `W`. Note that it seems to be \"backwards\", since it is designed to multiply vectors of length 2 (the input size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(2)\n",
    "model.W * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole `model` object represents a set of three sigmoidal neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ.(model.W*x + model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we have used Julia's **broadcasting** capability, in which the function $\\sigma$ is applied to each element of the vector `W * x` in turn. This elementwise application of the function is implicit in most of the literature on machine learning, but it is much clearer to make it explicit, as Julia allows us (in fact, basically forces us) to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y) = Flux.mse(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(params(model), 0.01)\n",
    "# give a list of the parameters that will be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:100\n",
    "    Flux.train!(loss, data, opt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the hyperplanes that the neurons have learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()\n",
    "\n",
    "contour!(0:0.01:1, 0:0.01:1, (x,y)->model([x,y]).data[1], levels=[0.5, 0.501], c=:blue)\n",
    "contour!(0:0.01:1, 0:0.01:1, (x,y)->model([x,y]).data[2], levels=[0.5,0.501], c=:red)\n",
    "contour!(0:0.01:1, 0:0.01:1, (x,y)->model([x,y]).data[3], levels=[0.5,0.501], c=:green)\n",
    "\n",
    "scatter!(first.(x_apples), last.(x_apples), m=:cross, label=\"apples\")\n",
    "scatter!(first.(x_bananas), last.(x_bananas), m=:circle, label=\"bananas\")\n",
    "scatter!(first.(x_grapes), last.(x_grapes), m=:square, label=\"grapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the result is not very good: this network is so simple that it is *not capable of learning a good representation of the data*. The reason for this is that the class of functions modelled is not complex enough.\n",
    "\n",
    "Note that two of the hyperplanes have been learnt correctly, the one that separates bananas from the rest, and the one that separates grapes from the rest. The third hyperplane has not, and it is intuitively clear why: there *is no way* to separate apples from non-apples with a *single* hyperplane, given this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
